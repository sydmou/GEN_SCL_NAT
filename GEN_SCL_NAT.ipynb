{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sydmou/GEN_SCL_NAT/blob/main/GEN_SCL_NAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yghILndFvE7d",
        "outputId": "77625d57-c8b5-4e41-a6a7-af5b44cfdbd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11\n"
          ]
        }
      ],
      "source": [
        "!python --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihs2CdEAvLL3",
        "outputId": "227bcd7e-5640-48bb-c7b9-50d77d304c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 16 09:31:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3HwFe8wvRd6",
        "outputId": "2d91b897-8de5-4ae5-ab53-19f960ff9ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.19.0\n",
            "  Downloading transformers-4.19.0-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.19.0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.19.0)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.0) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.12.1 transformers-4.19.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers==4.19.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GNj6DVTTvXhz"
      },
      "outputs": [],
      "source": [
        "!pip install torch>=1.10.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YPvtZOKvus2",
        "outputId": "93ce2e08-aef2-4e5a-8e30-2401424d58fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning==1.8.6\n",
            "  Downloading pytorch_lightning-1.8.6-py3-none-any.whl (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.3/800.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.6) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.6) (2.0.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.6) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.6) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.6) (2023.4.0)\n",
            "Collecting tensorboardX>=2.2 (from pytorch_lightning==1.8.6)\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.7.0 (from pytorch_lightning==1.8.6)\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.6) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.8.6) (4.5.0)\n",
            "Collecting lightning-utilities!=0.4.0,>=0.3.0 (from pytorch_lightning==1.8.6)\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.8.6) (2.27.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch_lightning==1.8.6)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.2->pytorch_lightning==1.8.6) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch_lightning==1.8.6) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch_lightning==1.8.6) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch_lightning==1.8.6) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch_lightning==1.8.6) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch_lightning==1.8.6) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->pytorch_lightning==1.8.6) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->pytorch_lightning==1.8.6) (16.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.6) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.6) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.6)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.6)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.6)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.6)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.8.6)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->pytorch_lightning==1.8.6) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.8.6) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.8.6) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.8.6) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->pytorch_lightning==1.8.6) (1.3.0)\n",
            "Installing collected packages: tensorboardX, multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch_lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch_lightning-1.8.6 tensorboardX-2.6 torchmetrics-0.11.4 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "pip install pytorch_lightning==1.8.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzu7OXc2v1JN",
        "outputId": "d5a6c05c-a955-4331-dada-47ca6bf70965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece==0.1.97\n",
            "  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece==0.1.97"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ft9IGfa4v9-E"
      },
      "outputs": [],
      "source": [
        "import numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amiEPxLlwLIT",
        "outputId": "8669084e-f445-433d-cddb-0a29094e1c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEBYPHXzwPKA",
        "outputId": "24f34751-c273-4593-a18f-8dff58c77de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/ColabNotebooks/ABSA-QUAD-master'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/ColabNotebooks/ABSA-QUAD-master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-b4q6hDwWYh"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f1iJ3VO4N7y",
        "outputId": "98b20c9b-bae4-4264-b7a8-a1283538435a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mK_FgmAwYyw",
        "outputId": "b0331195-465e-411b-d06e-8f2264ce2a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-15 10:03:25.031202: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-15 10:03:25.939768: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            " ============================== NEW EXP: ASQP on rest16 ============================== \n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "Here is an example (from the dev set):\n",
            "Input words: The pizza was pretty good and huge .\n",
            "Input tuples: [['pizza', 'food quality', 'positive', 'good'], ['pizza', 'food style_options', 'positive', 'huge']]\n",
            "Total examples = 316\n",
            "Input : Great friendly service, Fast seating, Fast Delivery, Excellent sushi.\n",
            "Output: service general is great because service is Great friendly [SSEP] service general is great because seating is Fast [SSEP] service general is great because Delivery is Fast [SSEP] food quality is great because sushi is Excellent\n",
            "\n",
            "****** Conduct Training ******\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus='1')` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices='1')` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\n",
            "  | Name  | Type                       | Params\n",
            "-----------------------------------------------------\n",
            "0 | model | T5ForConditionalGeneration | 222 M \n",
            "-----------------------------------------------------\n",
            "222 M     Trainable params\n",
            "0         Non-trainable params\n",
            "222 M     Total params\n",
            "891.614   Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]Input words: The pizza was pretty good and huge .\n",
            "Input tuples: [['pizza', 'food quality', 'positive', 'good'], ['pizza', 'food style_options', 'positive', 'huge']]\n",
            "Total examples = 316\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Input words: I had the duck breast special on my last visit and it was incredible .\n",
            "Input tuples: [['duck breast special', 'food quality', 'positive', 'incredible']]\n",
            "Total examples = 1264\n",
            "Epoch 0:  61% 60/99 [00:48<00:31,  1.24it/s, loss=0.506, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  81% 80/99 [01:04<00:15,  1.24it/s, loss=0.506, v_num=21]\n",
            "Epoch 0: 100% 99/99 [01:09<00:00,  1.42it/s, loss=0.436, v_num=21]\n",
            "Epoch 1:  61% 60/99 [00:52<00:34,  1.14it/s, loss=0.26, v_num=21] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  81% 80/99 [01:09<00:16,  1.15it/s, loss=0.26, v_num=21]\n",
            "Epoch 1: 100% 99/99 [01:15<00:00,  1.32it/s, loss=0.286, v_num=21]\n",
            "Epoch 2:  61% 60/99 [00:52<00:34,  1.14it/s, loss=0.197, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  81% 80/99 [01:09<00:16,  1.15it/s, loss=0.197, v_num=21]\n",
            "Epoch 2: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.2, v_num=21]  \n",
            "Epoch 3:  61% 60/99 [00:52<00:34,  1.14it/s, loss=0.116, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  81% 80/99 [01:09<00:16,  1.15it/s, loss=0.116, v_num=21]\n",
            "Epoch 3: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.132, v_num=21]\n",
            "Epoch 4:  61% 60/99 [00:53<00:34,  1.13it/s, loss=0.117, v_num=21] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  81% 80/99 [01:10<00:16,  1.14it/s, loss=0.117, v_num=21]\n",
            "Epoch 4: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.109, v_num=21]\n",
            "Epoch 5:  61% 60/99 [00:53<00:34,  1.13it/s, loss=0.0742, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  81% 80/99 [01:10<00:16,  1.14it/s, loss=0.0742, v_num=21]\n",
            "Epoch 5: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.0831, v_num=21]\n",
            "Epoch 6:  61% 60/99 [00:53<00:34,  1.12it/s, loss=0.0535, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  81% 80/99 [01:11<00:16,  1.12it/s, loss=0.0535, v_num=21]\n",
            "Epoch 6: 100% 99/99 [01:16<00:00,  1.29it/s, loss=0.0658, v_num=21]\n",
            "Epoch 7:  61% 60/99 [00:53<00:34,  1.12it/s, loss=0.057, v_num=21] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  81% 80/99 [01:10<00:16,  1.14it/s, loss=0.057, v_num=21]\n",
            "Epoch 7: 100% 99/99 [01:16<00:00,  1.30it/s, loss=0.0572, v_num=21]\n",
            "Epoch 8:  61% 60/99 [00:53<00:34,  1.13it/s, loss=0.0472, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  81% 80/99 [01:09<00:16,  1.14it/s, loss=0.0472, v_num=21]\n",
            "Epoch 8: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.0528, v_num=21]\n",
            "Epoch 9:  61% 60/99 [00:52<00:34,  1.13it/s, loss=0.0485, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  81% 80/99 [01:09<00:16,  1.14it/s, loss=0.0485, v_num=21]\n",
            "Epoch 9: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.0374, v_num=21]\n",
            "Epoch 10:  61% 60/99 [00:54<00:35,  1.10it/s, loss=0.0397, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  81% 80/99 [01:11<00:17,  1.12it/s, loss=0.0397, v_num=21]\n",
            "Epoch 10: 100% 99/99 [01:17<00:00,  1.28it/s, loss=0.031, v_num=21] \n",
            "Epoch 11:  61% 60/99 [00:53<00:34,  1.13it/s, loss=0.0325, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  81% 80/99 [01:09<00:16,  1.14it/s, loss=0.0325, v_num=21]\n",
            "Epoch 11: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.0396, v_num=21]\n",
            "Epoch 12:  61% 60/99 [00:54<00:35,  1.10it/s, loss=0.0368, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  81% 80/99 [01:11<00:16,  1.12it/s, loss=0.0368, v_num=21]\n",
            "Epoch 12: 100% 99/99 [01:17<00:00,  1.28it/s, loss=0.0262, v_num=21]\n",
            "Epoch 13:  61% 60/99 [00:52<00:34,  1.13it/s, loss=0.0238, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  81% 80/99 [01:09<00:16,  1.15it/s, loss=0.0238, v_num=21]\n",
            "Epoch 13: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.0217, v_num=21]\n",
            "Epoch 14:  61% 60/99 [01:02<00:40,  1.04s/it, loss=0.021, v_num=21] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  81% 80/99 [01:19<00:18,  1.01it/s, loss=0.021, v_num=21]\n",
            "Epoch 14: 100% 99/99 [01:25<00:00,  1.16it/s, loss=0.0246, v_num=21]\n",
            "Epoch 15:  61% 60/99 [00:53<00:34,  1.12it/s, loss=0.0285, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  81% 80/99 [01:10<00:16,  1.14it/s, loss=0.0285, v_num=21]\n",
            "Epoch 15: 100% 99/99 [01:15<00:00,  1.30it/s, loss=0.0304, v_num=21]\n",
            "Epoch 16:  61% 60/99 [00:54<00:35,  1.10it/s, loss=0.0225, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  81% 80/99 [01:12<00:17,  1.11it/s, loss=0.0225, v_num=21]\n",
            "Epoch 16: 100% 99/99 [01:17<00:00,  1.28it/s, loss=0.0167, v_num=21]\n",
            "Epoch 17:  61% 60/99 [00:52<00:34,  1.14it/s, loss=0.0174, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  81% 80/99 [01:09<00:16,  1.15it/s, loss=0.0174, v_num=21]\n",
            "Epoch 17: 100% 99/99 [01:15<00:00,  1.32it/s, loss=0.0197, v_num=21]\n",
            "Epoch 18:  61% 60/99 [00:52<00:34,  1.14it/s, loss=0.0221, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  81% 80/99 [01:09<00:16,  1.15it/s, loss=0.0221, v_num=21]\n",
            "Epoch 18: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.0131, v_num=21]\n",
            "Epoch 19:  61% 60/99 [00:53<00:34,  1.13it/s, loss=0.0668, v_num=21]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  81% 80/99 [01:09<00:16,  1.14it/s, loss=0.0668, v_num=21]\n",
            "Epoch 19: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.0352, v_num=21]\n",
            "Epoch 19: 100% 99/99 [01:15<00:00,  1.31it/s, loss=0.0352, v_num=21]`Trainer.fit` stopped: `max_epochs=20` reached.\n",
            "Epoch 19: 100% 99/99 [01:45<00:00,  1.07s/it, loss=0.0352, v_num=21]\n",
            "Finish training and saving the model!\n",
            "\n",
            "****** Conduct Evaluating with the last state ******\n",
            "Input words: Perfect on a cold day .\n",
            "Input tuples: [['NULL', 'food quality', 'positive', 'Perfect']]\n",
            "\n",
            "Input words: Perfect on a cold day .\n",
            "Input tuples: [['NULL', 'food quality', 'positive', 'Perfect']]\n",
            "Total examples = 544\n",
            "100% 17/17 [01:48<00:00,  6.38s/it]\n",
            "\n",
            "Results:\n",
            "number of gold spans: 799, predicted spans: 861, hit: 465\n",
            "{'precision': 0.5400696864111498, 'recall': 0.5819774718397998, 'f1': 0.5602409638554218}\n"
          ]
        }
      ],
      "source": [
        "!sh run.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xi4hEG0f6Rap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDfFqOYHxSQ5",
        "outputId": "8e37bba6-59c5-48a9-98f9-1b2264bc3ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting parse\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: parse\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24570 sha256=b22654b7aa6131fff4966b05767d05587331a2e80e9b6f9282624bb3cb4ccad4\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4b/f0/eaf5a8de646d8676dc25caa01949b9f9d883b8fa2efb435bc3\n",
            "Successfully built parse\n",
            "Installing collected packages: parse\n",
            "Successfully installed parse-1.19.0\n"
          ]
        }
      ],
      "source": [
        "pip install parse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-alkMJtJxDRj",
        "outputId": "8939b154-9ca6-4aba-e7d0-b92e5411828e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ColabNotebooks/GEN_SCL_NAT-main/source\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/ColabNotebooks/GEN_SCL_NAT-main/source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqrOVPXvxf8s",
        "outputId": "66a7b37d-e1e0-4272-93ed-2b81ced33afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "category_mappings.json  gen_scl_nat_main.py  Miniconda3-latest-Linux-x86_64.sh\n",
            "data_utils.py           \u001b[0m\u001b[01;34minference_outputs\u001b[0m/   \u001b[01;34m__pycache__\u001b[0m/\n",
            "eval_utils.py           infer_restaurant.sh  utils.py\n",
            "generate_data.py        losses.py\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4N0RoGLxE2Q",
        "outputId": "3c5a5e83-52c7-4f0d-f1bd-dd3c56b26304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-16 09:44:08.838426: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-16 09:44:10.688542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "acos_restaurant_dataIgen_scl_natIt5-baseIbeamsI5IwdI0.0Imax_epochsI5IesI0IaccI1IlrI9e-05Icont_lossI0.05Icont_tempI0.25ItruncIFalseIseedI123Irestaurant_output\n",
            "Global seed set to 123\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "Sentiment distribution\n",
            "Counter({2: 928, 0: 433, 3: 123, 1: 46})\n",
            "Here is an example (from the train set):\n",
            "Input : judging from previous posts this used to be a good place, but not any longer.\n",
            "tensor([    3, 24331,    45,  1767,  3489,    48,   261,    12,    36,     3,\n",
            "            9,   207,   286,     3,     6,    68,    59,   136,  1200,     3,\n",
            "            5,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Output: the restaurant overall | THE place IS not any longer | negative\n",
            "tensor([   8, 2062, 1879, 1820, 1853,  286, 6827,   59,  136, 1200, 1820, 2841,\n",
            "           1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0])\n",
            "\n",
            "****** Conducting Training ******\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus='1')` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices='1')` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: inference_outputs/acos_restaurant_dataIgen_scl_natIt5-baseIbeamsI5IwdI0.0Imax_epochsI5IesI0IaccI1IlrI9e-05Icont_lossI0.05Icont_tempI0.25ItruncIFalseIseedI123Irestaurant_output/lightning_logs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\n",
            "  | Name       | Type                       | Params\n",
            "----------------------------------------------------------\n",
            "0 | model      | T5ForConditionalGeneration | 222 M \n",
            "1 | cont_model | LinearModel                | 24.6 K\n",
            "2 | op_model   | LinearModel                | 24.6 K\n",
            "3 | as_model   | LinearModel                | 24.6 K\n",
            "4 | cat_model  | LinearModel                | 24.6 K\n",
            "----------------------------------------------------------\n",
            "222 M     Trainable params\n",
            "0         Non-trainable params\n",
            "222 M     Total params\n",
            "891.925   Total estimated model params size (MB)\n",
            "Sanity Checking: 0it [00:00, ?it/s]Sentiment distribution\n",
            "Counter({2: 113, 0: 41, 3: 13, 1: 4})\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:00<00:00,  2.33it/s]val_loss:\t tensor(9.3851, device='cuda:0')\n",
            "Sentiment distribution\n",
            "Counter({2: 928, 0: 433, 3: 123, 1: 46})\n",
            "Epoch 0:  75% 80/106 [01:12<00:23,  1.10it/s, loss=2.8, v_num=0] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  94% 100/106 [01:27<00:05,  1.14it/s, loss=2.8, v_num=0]\n",
            "Epoch 0: 100% 106/106 [01:29<00:00,  1.19it/s, loss=2.8, v_num=0]val_loss:\t tensor(2.2801, device='cuda:0')\n",
            "Epoch 0: 100% 106/106 [01:29<00:00,  1.19it/s, loss=2.59, v_num=0]\n",
            "Epoch 1:  75% 80/106 [01:13<00:23,  1.09it/s, loss=2.27, v_num=0]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  94% 100/106 [01:28<00:05,  1.13it/s, loss=2.27, v_num=0]\n",
            "Epoch 1: 100% 106/106 [01:30<00:00,  1.17it/s, loss=2.27, v_num=0]val_loss:\t tensor(2.0842, device='cuda:0')\n",
            "Epoch 1: 100% 106/106 [01:30<00:00,  1.17it/s, loss=2.22, v_num=0]\n",
            "Epoch 2:  75% 80/106 [01:13<00:23,  1.09it/s, loss=2.11, v_num=0]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  94% 100/106 [01:28<00:05,  1.13it/s, loss=2.11, v_num=0]\n",
            "Epoch 2: 100% 106/106 [01:30<00:00,  1.17it/s, loss=2.11, v_num=0]val_loss:\t tensor(2.0324, device='cuda:0')\n",
            "Epoch 2: 100% 106/106 [01:30<00:00,  1.17it/s, loss=2.1, v_num=0] \n",
            "Epoch 3:  75% 80/106 [01:15<00:24,  1.06it/s, loss=1.99, v_num=0]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  94% 100/106 [01:30<00:05,  1.10it/s, loss=1.99, v_num=0]\n",
            "Epoch 3: 100% 106/106 [01:32<00:00,  1.15it/s, loss=1.99, v_num=0]val_loss:\t tensor(2.0132, device='cuda:0')\n",
            "Epoch 3: 100% 106/106 [01:32<00:00,  1.15it/s, loss=1.97, v_num=0]\n",
            "Epoch 4:  75% 80/106 [01:13<00:23,  1.09it/s, loss=1.9, v_num=0] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  94% 100/106 [01:28<00:05,  1.13it/s, loss=1.9, v_num=0]\n",
            "Epoch 4: 100% 106/106 [01:30<00:00,  1.17it/s, loss=1.9, v_num=0]val_loss:\t tensor(1.9774, device='cuda:0')\n",
            "Epoch 4: 100% 106/106 [01:30<00:00,  1.17it/s, loss=1.87, v_num=0]\n",
            "Epoch 4: 100% 106/106 [01:30<00:00,  1.17it/s, loss=1.87, v_num=0]`Trainer.fit` stopped: `max_epochs=5` reached.\n",
            "Epoch 4: 100% 106/106 [01:45<00:00,  1.00it/s, loss=1.87, v_num=0]\n",
            "Finish training and saving the model!\n"
          ]
        }
      ],
      "source": [
        "!python gen_scl_nat_main.py \\\n",
        "   --task gen_scl_nat \\\n",
        "   --do_train\\\n",
        "   --dataset \\acos_restaurant_data \\\n",
        "   --model_name_or_path t5-base \\\n",
        "   --output_folder inference_outputs \\\n",
        "   --n_gpu 1 \\\n",
        "   --train_batch_size 16 \\\n",
        "   --eval_batch_size 16 \\\n",
        "   --learning_rate 9e-5 \\\n",
        "   --gradient_accumulation_steps 1 \\\n",
        "   --num_train_epochs 5 \\\n",
        "   --num_beams 5 \\\n",
        "   --weight_decay 0.0 \\\n",
        "   --seed 123 \\\n",
        "   --cont_loss 0.05 \\\n",
        "   --cont_temp 0.25 \\\n",
        "   --model_prefix restaurant_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ais2fvXyxd3l"
      },
      "source": [
        "# 新段落"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMeLdndnuw91yL+AyBwCv80",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}